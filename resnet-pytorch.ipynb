{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcd9041-feee-4094-9b20-601e71d03cf0",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "1fcd9041-feee-4094-9b20-601e71d03cf0",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd",
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: torchsummary\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed torchsummary-1.5.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cd29f5-f036-41a9-9e1e-f3c640ef0511",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "23cd29f5-f036-41a9-9e1e-f3c640ef0511",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88582880-b254-4ad8-b96d-e65b77cff139",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "88582880-b254-4ad8-b96d-e65b77cff139",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd"
    }
   },
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a175e0a-48ea-422f-ba39-04b9aa1f224e",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "9a175e0a-48ea-422f-ba39-04b9aa1f224e",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Conv2dAuto(nn.Conv2d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.padding = (self.kernel_size[0] // 2, self.kernel_size[1] // 2) # dynamic add padding based on the kernel_size\n",
    "\n",
    "conv3x3 = partial(Conv2dAuto, kernel_size=3, bias=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86c8daf-7202-4152-b736-e792811232e6",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "f86c8daf-7202-4152-b736-e792811232e6",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class ResNetGate(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        self.blocks = nn.Sequential(OrderedDict({\n",
    "            'conv': Conv2dAuto(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, bias=False)\n",
    "            ,'bn': nn.BatchNorm2d(out_channels)\n",
    "            ,'activation': nn.ReLU()\n",
    "            ,'mp': nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "            }))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.blocks(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15555715-ac19-447b-9b8a-00d12439b481",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "15555715-ac19-447b-9b8a-00d12439b481",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def conv_bn(in_channels, out_channels, conv, *args, **kwargs):\n",
    "    return nn.Sequential(OrderedDict({'conv': conv(in_channels, out_channels, *args, **kwargs), \n",
    "                          'bn': nn.BatchNorm2d(out_channels) }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51325b3e-1911-40d3-88d2-d6e82b98b4f1",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "51325b3e-1911-40d3-88d2-d6e82b98b4f1",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class ResNetBasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, conv=conv3x3, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        self.in_channels, self.out_channels, self.conv = in_channels, out_channels, conv\n",
    "\n",
    "        stride = 2 if in_channels != out_channels else 1\n",
    "\n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels, self.out_channels, kernel_size=1,\n",
    "                      stride=2, bias=False),\n",
    "            nn.BatchNorm2d(self.out_channels)\n",
    "        ) if self.apply_shortcut else None\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            conv_bn(self.in_channels, self.out_channels, self.conv, stride=stride)\n",
    "            ,activation()\n",
    "            ,conv_bn(self.out_channels, self.out_channels, self.conv)\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def apply_shortcut(self):\n",
    "        return self.in_channels != self.out_channels\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        if self.apply_shortcut: residual = self.shortcut(x)\n",
    "        x = self.blocks(x)\n",
    "        x += residual\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97be64e5-b5df-4c38-a7a9-0fe9c8e91b48",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "97be64e5-b5df-4c38-a7a9-0fe9c8e91b48",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class ResNetLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, block=ResNetBasicBlock, n=1, activation=nn.ReLU, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            block(in_channels, out_channels, activation=activation, *args, **kwargs),\n",
    "            *[block(out_channels, out_channels, activation=activation, *args, **kwargs) for n in range(n-1)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.blocks(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0a8ab1-e5d4-4a76-adf9-c6a0be6e42df",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "3b0a8ab1-e5d4-4a76-adf9-c6a0be6e42df",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class ResNetEncoder(nn.Module):\n",
    "    def __init__(self, in_channels=3, layer_sizes=[2,2,2,2], block_sizes=[64, 128, 256, 512]\n",
    "                ,activation=nn.ReLU, block=ResNetBasicBlock, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gate = ResNetGate(in_channels, block_sizes[0], kernel_size=7, stride=2)\n",
    "\n",
    "        self.in_out_block_sizes = list(zip(block_sizes, block_sizes[1:]))\n",
    "\n",
    "        self.encoder = nn.ModuleList([\n",
    "            ResNetLayer(block_sizes[0], block_sizes[0], block=block, n=layer_sizes[0], activation=activation)\n",
    "            ,*[ResNetLayer(in_channels, out_channels, block=block, n=depth, activation=activation) \n",
    "                for (in_channels, out_channels), depth in zip(self.in_out_block_sizes, layer_sizes[1:])]]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.gate(x)\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0d13cd-0982-4f3f-8234-3dd496d4fbde",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "7e0d13cd-0982-4f3f-8234-3dd496d4fbde",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class ResNetDecoder(nn.Module):\n",
    "    def __init__(self, in_features, n_classes, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.avgPool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.decoder = nn.Linear(in_features, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avgPool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7769ec25-bea6-4477-a091-49ccf9a7b767",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "7769ec25-bea6-4477-a091-49ccf9a7b767",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, in_channels, n_classes, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = ResNetEncoder(in_channels=in_channels, *args, **kwargs)\n",
    "        self.decoder = ResNetDecoder(self.encoder.encoder[-1].blocks[-1].out_channels, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9657a4-12e4-4433-a9e3-37a6a9337eb8",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "bb9657a4-12e4-4433-a9e3-37a6a9337eb8",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def resnet34(in_channels, n_classes):\n",
    "    return ResNet(in_channels, n_classes, block=ResNetBasicBlock, layer_sizes=[3, 4, 6, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946a653a-58d1-443a-8877-993c9b2d6470",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "946a653a-58d1-443a-8877-993c9b2d6470",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd",
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image = torch.Tensor(np.ones((1,3,224,224)))\n",
    "\n",
    "resnet34_test = resnet34(3, 10)\n",
    "resnet34_test(test_image).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9a0802-2cb8-4a0b-9acc-5f60bb19a4eb",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "8b9a0802-2cb8-4a0b-9acc-5f60bb19a4eb",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd"
    }
   },
   "source": [
    "# Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e6450e-0447-48a6-b224-cceb211f6ddb",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "28e6450e-0447-48a6-b224-cceb211f6ddb",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd",
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "        Conv2dAuto-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "        ResNetGate-5           [-1, 64, 56, 56]               0\n",
      "        Conv2dAuto-6           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-7           [-1, 64, 56, 56]             128\n",
      "              ReLU-8           [-1, 64, 56, 56]               0\n",
      "        Conv2dAuto-9           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-10           [-1, 64, 56, 56]             128\n",
      " ResNetBasicBlock-11           [-1, 64, 56, 56]               0\n",
      "       Conv2dAuto-12           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
      "             ReLU-14           [-1, 64, 56, 56]               0\n",
      "       Conv2dAuto-15           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
      " ResNetBasicBlock-17           [-1, 64, 56, 56]               0\n",
      "       Conv2dAuto-18           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-19           [-1, 64, 56, 56]             128\n",
      "             ReLU-20           [-1, 64, 56, 56]               0\n",
      "       Conv2dAuto-21           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-22           [-1, 64, 56, 56]             128\n",
      " ResNetBasicBlock-23           [-1, 64, 56, 56]               0\n",
      "      ResNetLayer-24           [-1, 64, 56, 56]               0\n",
      "           Conv2d-25          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-26          [-1, 128, 28, 28]             256\n",
      "       Conv2dAuto-27          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-28          [-1, 128, 28, 28]             256\n",
      "             ReLU-29          [-1, 128, 28, 28]               0\n",
      "       Conv2dAuto-30          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-31          [-1, 128, 28, 28]             256\n",
      " ResNetBasicBlock-32          [-1, 128, 28, 28]               0\n",
      "       Conv2dAuto-33          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-34          [-1, 128, 28, 28]             256\n",
      "             ReLU-35          [-1, 128, 28, 28]               0\n",
      "       Conv2dAuto-36          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-37          [-1, 128, 28, 28]             256\n",
      " ResNetBasicBlock-38          [-1, 128, 28, 28]               0\n",
      "       Conv2dAuto-39          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-40          [-1, 128, 28, 28]             256\n",
      "             ReLU-41          [-1, 128, 28, 28]               0\n",
      "       Conv2dAuto-42          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-43          [-1, 128, 28, 28]             256\n",
      " ResNetBasicBlock-44          [-1, 128, 28, 28]               0\n",
      "       Conv2dAuto-45          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-46          [-1, 128, 28, 28]             256\n",
      "             ReLU-47          [-1, 128, 28, 28]               0\n",
      "       Conv2dAuto-48          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-49          [-1, 128, 28, 28]             256\n",
      " ResNetBasicBlock-50          [-1, 128, 28, 28]               0\n",
      "      ResNetLayer-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-53          [-1, 256, 14, 14]             512\n",
      "       Conv2dAuto-54          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-55          [-1, 256, 14, 14]             512\n",
      "             ReLU-56          [-1, 256, 14, 14]               0\n",
      "       Conv2dAuto-57          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-58          [-1, 256, 14, 14]             512\n",
      " ResNetBasicBlock-59          [-1, 256, 14, 14]               0\n",
      "       Conv2dAuto-60          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-61          [-1, 256, 14, 14]             512\n",
      "             ReLU-62          [-1, 256, 14, 14]               0\n",
      "       Conv2dAuto-63          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-64          [-1, 256, 14, 14]             512\n",
      " ResNetBasicBlock-65          [-1, 256, 14, 14]               0\n",
      "       Conv2dAuto-66          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-67          [-1, 256, 14, 14]             512\n",
      "             ReLU-68          [-1, 256, 14, 14]               0\n",
      "       Conv2dAuto-69          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-70          [-1, 256, 14, 14]             512\n",
      " ResNetBasicBlock-71          [-1, 256, 14, 14]               0\n",
      "       Conv2dAuto-72          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-73          [-1, 256, 14, 14]             512\n",
      "             ReLU-74          [-1, 256, 14, 14]               0\n",
      "       Conv2dAuto-75          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-76          [-1, 256, 14, 14]             512\n",
      " ResNetBasicBlock-77          [-1, 256, 14, 14]               0\n",
      "       Conv2dAuto-78          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-79          [-1, 256, 14, 14]             512\n",
      "             ReLU-80          [-1, 256, 14, 14]               0\n",
      "       Conv2dAuto-81          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-82          [-1, 256, 14, 14]             512\n",
      " ResNetBasicBlock-83          [-1, 256, 14, 14]               0\n",
      "       Conv2dAuto-84          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-85          [-1, 256, 14, 14]             512\n",
      "             ReLU-86          [-1, 256, 14, 14]               0\n",
      "       Conv2dAuto-87          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-88          [-1, 256, 14, 14]             512\n",
      " ResNetBasicBlock-89          [-1, 256, 14, 14]               0\n",
      "      ResNetLayer-90          [-1, 256, 14, 14]               0\n",
      "           Conv2d-91            [-1, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-92            [-1, 512, 7, 7]           1,024\n",
      "       Conv2dAuto-93            [-1, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-94            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-95            [-1, 512, 7, 7]               0\n",
      "       Conv2dAuto-96            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-97            [-1, 512, 7, 7]           1,024\n",
      " ResNetBasicBlock-98            [-1, 512, 7, 7]               0\n",
      "       Conv2dAuto-99            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-100            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-101            [-1, 512, 7, 7]               0\n",
      "      Conv2dAuto-102            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-103            [-1, 512, 7, 7]           1,024\n",
      "ResNetBasicBlock-104            [-1, 512, 7, 7]               0\n",
      "      Conv2dAuto-105            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-106            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-107            [-1, 512, 7, 7]               0\n",
      "      Conv2dAuto-108            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-109            [-1, 512, 7, 7]           1,024\n",
      "ResNetBasicBlock-110            [-1, 512, 7, 7]               0\n",
      "     ResNetLayer-111            [-1, 512, 7, 7]               0\n",
      "   ResNetEncoder-112            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-113            [-1, 512, 1, 1]               0\n",
      "          Linear-114                   [-1, 10]           5,130\n",
      "   ResNetDecoder-115                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 21,289,802\n",
      "Trainable params: 21,289,802\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 90.35\n",
      "Params size (MB): 81.21\n",
      "Estimated Total Size (MB): 172.14\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(resnet34_test.cuda(), (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f74a1d-7d5e-4cdb-805f-51edf2d47063",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "79f74a1d-7d5e-4cdb-805f-51edf2d47063",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd",
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
      "             ReLU-14           [-1, 64, 56, 56]               0\n",
      "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
      "             ReLU-17           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-20           [-1, 64, 56, 56]             128\n",
      "             ReLU-21           [-1, 64, 56, 56]               0\n",
      "           Conv2d-22           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-23           [-1, 64, 56, 56]             128\n",
      "             ReLU-24           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-25           [-1, 64, 56, 56]               0\n",
      "           Conv2d-26          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-27          [-1, 128, 28, 28]             256\n",
      "             ReLU-28          [-1, 128, 28, 28]               0\n",
      "           Conv2d-29          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-30          [-1, 128, 28, 28]             256\n",
      "           Conv2d-31          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
      "             ReLU-33          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-36          [-1, 128, 28, 28]             256\n",
      "             ReLU-37          [-1, 128, 28, 28]               0\n",
      "           Conv2d-38          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-39          [-1, 128, 28, 28]             256\n",
      "             ReLU-40          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-41          [-1, 128, 28, 28]               0\n",
      "           Conv2d-42          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-43          [-1, 128, 28, 28]             256\n",
      "             ReLU-44          [-1, 128, 28, 28]               0\n",
      "           Conv2d-45          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-46          [-1, 128, 28, 28]             256\n",
      "             ReLU-47          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-48          [-1, 128, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-55          [-1, 128, 28, 28]               0\n",
      "           Conv2d-56          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-57          [-1, 256, 14, 14]             512\n",
      "             ReLU-58          [-1, 256, 14, 14]               0\n",
      "           Conv2d-59          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-60          [-1, 256, 14, 14]             512\n",
      "           Conv2d-61          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-62          [-1, 256, 14, 14]             512\n",
      "             ReLU-63          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-64          [-1, 256, 14, 14]               0\n",
      "           Conv2d-65          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-66          [-1, 256, 14, 14]             512\n",
      "             ReLU-67          [-1, 256, 14, 14]               0\n",
      "           Conv2d-68          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-69          [-1, 256, 14, 14]             512\n",
      "             ReLU-70          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-71          [-1, 256, 14, 14]               0\n",
      "           Conv2d-72          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-73          [-1, 256, 14, 14]             512\n",
      "             ReLU-74          [-1, 256, 14, 14]               0\n",
      "           Conv2d-75          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-76          [-1, 256, 14, 14]             512\n",
      "             ReLU-77          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-78          [-1, 256, 14, 14]               0\n",
      "           Conv2d-79          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-80          [-1, 256, 14, 14]             512\n",
      "             ReLU-81          [-1, 256, 14, 14]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-85          [-1, 256, 14, 14]               0\n",
      "           Conv2d-86          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-87          [-1, 256, 14, 14]             512\n",
      "             ReLU-88          [-1, 256, 14, 14]               0\n",
      "           Conv2d-89          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-90          [-1, 256, 14, 14]             512\n",
      "             ReLU-91          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-92          [-1, 256, 14, 14]               0\n",
      "           Conv2d-93          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-94          [-1, 256, 14, 14]             512\n",
      "             ReLU-95          [-1, 256, 14, 14]               0\n",
      "           Conv2d-96          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-97          [-1, 256, 14, 14]             512\n",
      "             ReLU-98          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-99          [-1, 256, 14, 14]               0\n",
      "          Conv2d-100            [-1, 512, 7, 7]       1,179,648\n",
      "     BatchNorm2d-101            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-102            [-1, 512, 7, 7]               0\n",
      "          Conv2d-103            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-104            [-1, 512, 7, 7]           1,024\n",
      "          Conv2d-105            [-1, 512, 7, 7]         131,072\n",
      "     BatchNorm2d-106            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-107            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-108            [-1, 512, 7, 7]               0\n",
      "          Conv2d-109            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-110            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-111            [-1, 512, 7, 7]               0\n",
      "          Conv2d-112            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-113            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-114            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-115            [-1, 512, 7, 7]               0\n",
      "          Conv2d-116            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-117            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-118            [-1, 512, 7, 7]               0\n",
      "          Conv2d-119            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-120            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-121            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-122            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-123            [-1, 512, 1, 1]               0\n",
      "          Linear-124                 [-1, 1000]         513,000\n",
      "================================================================\n",
      "Total params: 21,797,672\n",
      "Trainable params: 21,797,672\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 96.29\n",
      "Params size (MB): 83.15\n",
      "Estimated Total Size (MB): 180.01\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "summary(models.resnet34(False).cuda(), (3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3726c95b-0ec7-43d7-9683-91415a0e8cef",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "3726c95b-0ec7-43d7-9683-91415a0e8cef",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd"
    }
   },
   "source": [
    "# Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac856c4a-efd8-4279-bbc4-87576f24697a",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "ac856c4a-efd8-4279-bbc4-87576f24697a",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246b781b-8f06-46a8-93f5-ab4bb92b1d0a",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "246b781b-8f06-46a8-93f5-ab4bb92b1d0a",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd",
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e6d5e2-0872-4923-85ca-63bc3c9b6b7a",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "e8e6d5e2-0872-4923-85ca-63bc3c9b6b7a",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd",
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_pil = trainset[0][0]\n",
    "\n",
    "plt.hist(np.array(img_pil).ravel(), bins=50, density=True);\n",
    "plt.xlabel(\"pixel values\")\n",
    "plt.ylabel(\"relative frequency\")\n",
    "plt.title(\"distribution of pixels\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93d73e0-8d17-453a-9713-63a54a9f9eb1",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "d93d73e0-8d17-453a-9713-63a54a9f9eb1",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd",
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "testset, valset = random_split(testset,[8000,2000])\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4355702d-d492-4593-b103-da6dde604a8c",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "4355702d-d492-4593-b103-da6dde604a8c",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd",
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee03dd20-a923-4e8d-91d8-461525d520ac",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "ee03dd20-a923-4e8d-91d8-461525d520ac",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "testset, new_trainset = random_split(trainset,[46000,4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2a0c97-7efd-4f51-86b8-a2932106a3fa",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "ba2a0c97-7efd-4f51-86b8-a2932106a3fa",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd",
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_pil2 = trainset[0][0]\n",
    "\n",
    "plt.hist(np.array(img_pil2).ravel(), bins=50, density=True);\n",
    "plt.xlabel(\"pixel values\")\n",
    "plt.ylabel(\"relative frequency\")\n",
    "plt.title(\"distribution of pixels\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bb865e-a912-488e-b4e7-bb26ab361e7e",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "73bb865e-a912-488e-b4e7-bb26ab361e7e",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd"
    }
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8203ec82-cc82-4d22-92b5-1f39bc9ab031",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "8203ec82-cc82-4d22-92b5-1f39bc9ab031",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(resnet34_test.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4618caf-5949-43b9-8088-83df65b230de",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "c4618caf-5949-43b9-8088-83df65b230de",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd",
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e5dc3c-77e2-4ef7-8c4b-e16cf32c71cc",
   "metadata": {
    "collapsed": true,
    "gradient": {
     "editing": false,
     "id": "52e5dc3c-77e2-4ef7-8c4b-e16cf32c71cc",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd",
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (encoder): ResNetEncoder(\n",
       "    (gate): ResNetGate(\n",
       "      (blocks): Sequential(\n",
       "        (conv): Conv2dAuto(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ReLU()\n",
       "        (mp): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (encoder): ModuleList(\n",
       "      (0): ResNetLayer(\n",
       "        (blocks): Sequential(\n",
       "          (0): ResNetBasicBlock(\n",
       "            (blocks): Sequential(\n",
       "              (0): Sequential(\n",
       "                (conv): Conv2dAuto(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (1): ReLU()\n",
       "              (2): Sequential(\n",
       "                (conv): Conv2dAuto(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): ResNetBasicBlock(\n",
       "            (blocks): Sequential(\n",
       "              (0): Sequential(\n",
       "                (conv): Conv2dAuto(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (1): ReLU()\n",
       "              (2): Sequential(\n",
       "                (conv): Conv2dAuto(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): ResNetBasicBlock(\n",
       "            (blocks): Sequential(\n",
       "              (0): Sequential(\n",
       "                (conv): Conv2dAuto(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (1): ReLU()\n",
       "              (2): Sequential(\n",
       "                (conv): Conv2dAuto(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): ResNetLayer(\n",
       "        (blocks): Sequential(\n",
       "          (0): ResNetBasicBlock(\n",
       "            (shortcut): Sequential(\n",
       "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (blocks): Sequential(\n",
       "              (0): Sequential(\n",
       "                (conv): Conv2dAuto(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (1): ReLU()\n",
       "              (2): Sequential(\n",
       "                (conv): Conv2dAuto(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): ResNetBasicBlock(\n",
       "            (blocks): Sequential(\n",
       "              (0): Sequential(\n",
       "                (conv): Conv2dAuto(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (1): ReLU()\n",
       "              (2): Sequential(\n",
       "                (conv): Conv2dAuto(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): ResNetBasicBlock(\n",
       "            (blocks): Sequential(\n",
       "              (0): Sequential(\n",
       "                (conv): Conv2dAuto(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (1): ReLU()\n",
       "              (2): Sequential(\n",
       "                (conv): Conv2dAuto(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): ResNetBasicBlock(\n",
       "            (blocks): Sequential(\n",
       "              (0): Sequential(\n",
       "                (conv): Conv2dAuto(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (1): ReLU()\n",
       "              (2): Sequential(\n",
       "                (conv): Conv2dAuto(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): ResNetLayer(\n",
       "        (blocks): Sequential(\n",
       "          (0): ResNetBasicBlock(\n",
       "            (shortcut): Sequential(\n",
       "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (blocks): Sequential(\n",
       "              (0): Sequential(\n",
       "                (conv): Conv2dAuto(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (1): ReLU()\n",
       "              (2): Sequential(\n",
       "                (conv): Conv2dAuto(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): ResNetBasicBlock(\n",
       "            (blocks): Sequential(\n",
       "              (0): Sequential(\n",
       "                (conv): Conv2dAuto(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (1): ReLU()\n",
       "              (2): Sequential(\n",
       "                (conv): Conv2dAuto(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): ResNetBasicBlock(\n",
       "            (blocks): Sequential(\n",
       "              (0): Sequential(\n",
       "                (conv): Conv2dAuto(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (1): ReLU()\n",
       "              (2): Sequential(\n",
       "                (conv): Conv2dAuto(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): ResNetBasicBlock(\n",
       "            (blocks): Sequential(\n",
       "              (0): Sequential(\n",
       "                (conv): Conv2dAuto(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (1): ReLU()\n",
       "              (2): Sequential(\n",
       "                (conv): Conv2dAuto(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (4): ResNetBasicBlock(\n",
       "            (blocks): Sequential(\n",
       "              (0): Sequential(\n",
       "                (conv): Conv2dAuto(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (1): ReLU()\n",
       "              (2): Sequential(\n",
       "                (conv): Conv2dAuto(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (5): ResNetBasicBlock(\n",
       "            (blocks): Sequential(\n",
       "              (0): Sequential(\n",
       "                (conv): Conv2dAuto(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (1): ReLU()\n",
       "              (2): Sequential(\n",
       "                (conv): Conv2dAuto(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): ResNetLayer(\n",
       "        (blocks): Sequential(\n",
       "          (0): ResNetBasicBlock(\n",
       "            (shortcut): Sequential(\n",
       "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (blocks): Sequential(\n",
       "              (0): Sequential(\n",
       "                (conv): Conv2dAuto(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (1): ReLU()\n",
       "              (2): Sequential(\n",
       "                (conv): Conv2dAuto(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): ResNetBasicBlock(\n",
       "            (blocks): Sequential(\n",
       "              (0): Sequential(\n",
       "                (conv): Conv2dAuto(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (1): ReLU()\n",
       "              (2): Sequential(\n",
       "                (conv): Conv2dAuto(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): ResNetBasicBlock(\n",
       "            (blocks): Sequential(\n",
       "              (0): Sequential(\n",
       "                (conv): Conv2dAuto(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (1): ReLU()\n",
       "              (2): Sequential(\n",
       "                (conv): Conv2dAuto(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): ResNetDecoder(\n",
       "    (avgPool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (decoder): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet34_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bed115-2374-4602-ba44-3013a5dbcd42",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "d1bed115-2374-4602-ba44-3013a5dbcd42",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd",
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  4000] loss: 2.055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  6000] loss: 1.907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  8000] loss: 1.827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 10000] loss: 1.780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 12000] loss: 1.744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        # inputs, labels = data\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = resnet34_test(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a15556-f61f-410a-9b29-8b54995d6bf4",
   "metadata": {
    "collapsed": true,
    "gradient": {
     "editing": true,
     "id": "09a15556-f61f-410a-9b29-8b54995d6bf4",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd",
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_98/745836985.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet34_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_98/1172434501.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_98/2736478359.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_98/3964775998.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;31m# TODO: if statement only here to tell the jit to skip emitting this when it is None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore[has-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# type: ignore[has-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# use cumulative moving average\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                     \u001b[0mexponential_average_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1221\u001b[0m                                         \u001b[0;34m\"(torch.Tensor or None expected)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m                                         .format(torch.typename(value), name))\n\u001b[0;32m-> 1223\u001b[0;31m                     \u001b[0mbuffers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m                     \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = []\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    train_loss = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        epoch_loss = 0\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        # inputs, labels = data\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = resnet34_test(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            train_loss += running_loss / 2000\n",
    "            running_loss = 0.0\n",
    "\n",
    "    val_loss = 0\n",
    "    for i, data in enumerate(valloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        outputs = resnet34_test(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss\n",
    "\n",
    "    epochs.append(epoch+1)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss / len(valset))\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     running_loss = 0.0\n",
    "#     for i, data in enumerate(trainloader, 0):\n",
    "#         running_loss =+ loss.item() * images.size(0)\n",
    "\n",
    "#     loss_values.append(running_loss / len(train_dataset))\n",
    "\n",
    "# plt.plot(loss_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760556b8-3b9b-4565-be1a-3adc9cceee95",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "760556b8-3b9b-4565-be1a-3adc9cceee95",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd"
    }
   },
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626b75dc-afc3-4134-aaec-90d6d9c623b2",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "626b75dc-afc3-4134-aaec-90d6d9c623b2",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "PATH = './cifar_resnet.pth'\n",
    "torch.save(resnet34_test.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d7d5d0-5bc4-4c88-81b2-f9a56ea1af0b",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "b5d7d5d0-5bc4-4c88-81b2-f9a56ea1af0b",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!ls cifar_resnet.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e0b027-4c25-477b-879e-123695d93682",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "f1e0b027-4c25-477b-879e-123695d93682",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd"
    }
   },
   "source": [
    "# Visualize Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c871191f-1eea-42ad-aba2-68da6aed1804",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "c871191f-1eea-42ad-aba2-68da6aed1804",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "loss_val = []\n",
    "\n",
    "for tensor in val_losses:\n",
    "    loss_val.append(tensor.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef79b92-7d61-4a1f-9779-10368eed55a3",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "1ef79b92-7d61-4a1f-9779-10368eed55a3",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd",
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d329cae6-e708-4c7b-8de3-8912c7eb8bfc",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "d329cae6-e708-4c7b-8de3-8912c7eb8bfc",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd",
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_98/3039568422.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'g'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'validation loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training and Validation loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_train' is not defined"
     ]
    }
   ],
   "source": [
    "# loss_train = np.array(train_losses)\n",
    "# loss_val = np.array(torch.Tensor.tocpu(val_losses))\n",
    "# epochs = range(1,35)\n",
    "\n",
    "\n",
    "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
    "plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04100b16-c7d7-473f-a503-d36c3d78bf95",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "04100b16-c7d7-473f-a503-d36c3d78bf95",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd"
    }
   },
   "source": [
    "# Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d9d49b-a76c-4895-8f51-989a603f228f",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "e3d9d49b-a76c-4895-8f51-989a603f228f",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd",
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet34_test = resnet34(3, 10)\n",
    "resnet34_test.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b510bd-948f-4bd2-8da1-918090896880",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "78b510bd-948f-4bd2-8da1-918090896880",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd",
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_98/3558230591.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# calculate outputs by running images through the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet34_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;31m# the class with the highest energy is what we choose as prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_98/1172434501.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_98/2736478359.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_98/3964775998.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 442\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "## Pretrained Model\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = resnet34_test(inputs)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb9cb84-4799-4548-aced-b9d61e5b49b4",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "3fb9cb84-4799-4548-aced-b9d61e5b49b4",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd",
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resnet34_test2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_98/273918650.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# calculate outputs by running images through the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet34_test2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;31m# the class with the highest energy is what we choose as prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'resnet34_test2' is not defined"
     ]
    }
   ],
   "source": [
    "## Untrained Model\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = resnet34_test2(inputs)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78758b45-0ce2-4d80-a8a4-8cb8a197e5b7",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "78758b45-0ce2-4d80-a8a4-8cb8a197e5b7",
     "kernelId": "1243023c-79c5-4602-af2a-61dba05859cd",
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_98/265734880.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet34_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# collect the correct predictions for each class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_98/1172434501.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_98/2736478359.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_98/3964775998.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 442\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = resnet34_test(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
    "                                                   accuracy))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
